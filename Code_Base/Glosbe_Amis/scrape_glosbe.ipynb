{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Scrape Glosbe:\n","\n","Glosbe does not let you see all of their sentences, only ones through certain queries. By querying the top 50 words we should be able to get about 91% of sentences, and then remove duplicates."],"metadata":{"id":"JD6HBncuJOyK"}},{"cell_type":"code","source":["!pip install deepl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gklB7JZVSTYy","executionInfo":{"status":"ok","timestamp":1712874679086,"user_tz":240,"elapsed":8665,"user":{"displayName":"Hunter Scheppat","userId":"01440848241027272383"}},"outputId":"9569686f-e980-4a6e-e1a0-0420e705ae18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deepl\n","  Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2024.2.2)\n","Installing collected packages: deepl\n","Successfully installed deepl-1.17.0\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Define the path to the files\n","path_to_files = '/content/drive/MyDrive/formosan_mt_project/translations/amis_videos/*indigenous.txt'\n","\n","# Define the top 50 words\n","word_freq_list = [('to', 10711), ('i', 10252), ('ko', 10188), ('a', 8331), ('no', 7030), ('o', 5346), ('sa', 3452), ('haw', 3330), ('ku', 3062), ('han', 2867), ('tu', 2765), ('ako', 2564), ('ira', 2501), ('kako', 2426), ('nu', 2137), ('u', 2132), ('ho', 2074), ('hay', 1577), ('ano', 1516), ('kora', 1483), ('caay', 1423), ('itini', 1379), ('itiya', 1350), ('kiya', 1335), ('nira', 1251), ('kami', 1218), ('mako', 1144), ('kira', 1133), ('niyam', 1108), ('saan', 1058), ('wawa', 1019), ('ci', 1017), ('san', 994), ('saka', 980), ('sato', 972), ('ya', 969), ('awaay', 938), ('sanay', 917), ('ka', 898), ('hananay', 884), ('kita', 827), ('mita', 781), ('ta', 771), ('matoÊ¼asay', 768), ('sowal', 761), ('niyaro', 742), ('aca', 730), ('anini', 674), ('tayra', 670), ('ha', 665)]\n","top_words = {word for word, freq in word_freq_list}\n","\n","\n","# Initialize counters\n","total_lines = 0\n","lines_with_top_words = 0\n","\n","# Process each file\n","for filepath in glob.glob(path_to_files):\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            total_lines += 1\n","            # Check if any of the top words is in the current line\n","            if any(word in line.lower().split() for word in top_words):\n","                lines_with_top_words += 1\n","\n","# Calculate the overall percentage\n","if total_lines > 0:  # Check to avoid division by zero\n","    percentage = (lines_with_top_words / total_lines) * 100\n","    print(f\"Overall, {percentage:.2f}% of lines contain at least one of the top words across all files.\")\n","else:\n","    print(\"No lines were found in the files.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJNjKu4tI8FB","executionInfo":{"status":"ok","timestamp":1712874560700,"user_tz":240,"elapsed":7699,"user":{"displayName":"Hunter Scheppat","userId":"01440848241027272383"}},"outputId":"62e135b8-3d56-4eb8-e4ca-a3067cabde41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Overall, 91.64% of lines contain at least one of the top words across all files.\n"]}]},{"cell_type":"markdown","source":["## Scrape the page"],"metadata":{"id":"Jt8XLxOUJf0g"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Initialize base URL and top words\n","base_url = \"https://glosbe.com/ami/zh/\"\n","\n","# Paths for the output files\n","indigenous_path = '/content/drive/MyDrive/formosan_mt_project/translations/amis_glosbe/glosbe-indigenous.txt'\n","chinese_path = '/content/drive/MyDrive/formosan_mt_project/translations/amis_glosbe/glosbe-chinese.txt'\n","\n","# Helper function to save sentences to files ensuring parallel corpus structure\n","def save_sentences(indigenous_sentences, chinese_sentences):\n","    with open(indigenous_path, 'a', encoding='utf-8') as fi, open(chinese_path, 'a', encoding='utf-8') as fc:\n","        for ind, chi in zip(indigenous_sentences, chinese_sentences):\n","            fi.write(ind + '\\n')\n","            fc.write(chi + '\\n')\n","\n","# Function to process each word and maintain parallel structure\n","def scrape_for_word(word):\n","    url = base_url + word\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Initialize data structures to track sentences and ensure uniqueness\n","    seen_pairs = set()\n","    indigenous_sentences = []\n","    chinese_sentences = []\n","\n","    # Internal function to process a page\n","    def process_page(soup):\n","        divs = soup.find_all('div', class_='flex')\n","        for div in divs:\n","            ami_text = div.find('div', attrs={'lang': 'ami'})\n","            zh_text = div.find('div', attrs={'lang': 'zh'})\n","            if ami_text and zh_text:\n","                ami_sentence = ami_text.text.strip()\n","                zh_sentence = zh_text.text.strip()\n","                pair = (ami_sentence, zh_sentence)\n","                if pair not in seen_pairs:\n","                    seen_pairs.add(pair)\n","                    indigenous_sentences.append(ami_sentence)\n","                    chinese_sentences.append(zh_sentence)\n","\n","    process_page(soup)\n","\n","    # Manage \"Load More\" functionality\n","    load_more = soup.find('button', attrs={'data-element': 'fragment-loader'})\n","    while load_more:\n","        more_url = 'https://glosbe.com' + load_more['data-fragment-url']\n","        response = requests.get(more_url)\n","        more_soup = BeautifulSoup(response.text, 'html.parser')\n","        process_page(more_soup)\n","        load_more = more_soup.find('button', attrs={'data-element': 'fragment-loader'})\n","\n","    # Save the sentences ensuring parallel structure\n","    save_sentences(indigenous_sentences, chinese_sentences)\n","\n","# Processing each word\n","for word in top_words:\n","    scrape_for_word(word)\n","\n","print(\"Scraping complete. Files saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfGGJNanJjKm","executionInfo":{"status":"ok","timestamp":1712874443868,"user_tz":240,"elapsed":28742,"user":{"displayName":"Hunter Scheppat","userId":"01440848241027272383"}},"outputId":"7ba4749b-6bc3-4c22-c3ef-5afb2c515ffe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Scraping complete. Files saved.\n"]}]},{"cell_type":"code","source":["# translate\n","import os\n","import deepl\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Replace with the path to your directory containing the files to be translated\n","directory_path = '/content/drive/MyDrive/formosan_mt_project/translations/amis_glosbe/'\n","\n","# DO NOT TAKE MY API KEY PLEASE!!! :)\n","auth_key = \"967acf8a-f24c-46d7-bf21-ea631153f8c9:fx\"\n","translator = deepl.Translator(auth_key)\n","\n","def translate_file(file_path):\n","    base_name = os.path.basename(file_path)\n","    file_number = base_name.split('-')[0]  # Assumes the file name format is \"number-chinese.txt\"\n","    output_file_name = f\"{file_number}-english.txt\"\n","    output_file_path = os.path.join(directory_path, output_file_name)\n","\n","    # Check if the English translation file already exists\n","    if os.path.exists(output_file_path):\n","        print(f\"Skipping translation for {file_number}: English file already exists.\")\n","        return\n","\n","    lines_to_translate = []\n","    with open(file_path, 'r', encoding='utf-8') as input_file:\n","        lines_to_translate = [line.strip() for line in input_file.readlines() if line.strip()]  # Skip empty lines\n","\n","    # DeepL API supports up to 50 texts in one request\n","    batch_size = 50\n","    translated_lines = []\n","    for i in range(0, len(lines_to_translate), batch_size):\n","        batch = lines_to_translate[i:i+batch_size]\n","        results = translator.translate_text(batch, source_lang=\"ZH\", target_lang=\"EN-US\")\n","        translated_lines.extend([result.text for result in results])\n","\n","    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","        output_file.write('\\n'.join(translated_lines))\n","\n","for file in os.listdir(directory_path):\n","    if file.endswith(\"-chinese.txt\"):\n","        file_path = os.path.join(directory_path, file)\n","        print(f\"Processing {file}...\")\n","        translate_file(file_path)\n","        print(f\"Finished processing {file}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXpnxa1aSQKd","executionInfo":{"status":"ok","timestamp":1712874865236,"user_tz":240,"elapsed":18359,"user":{"displayName":"Hunter Scheppat","userId":"01440848241027272383"}},"outputId":"907efb06-d0dc-4ca2-cd0e-801ce4dc845d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Processing glosbe-chinese.txt...\n","Finished processing glosbe-chinese.txt.\n"]}]}]}